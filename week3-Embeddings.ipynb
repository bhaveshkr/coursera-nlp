{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "#download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(datapath(cwd+'/GoogleNews-vectors-negative300.bin'), binary=True, limit=500000)  # C bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin     data\t  week3-Embeddings.ipynb\r\n",
      "GoogleNews-vectors-negative300.bin.gz  grader.py\r\n",
      "__pycache__\t\t\t       util.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    words_embedding_score = np.zeros(dim)\n",
    "    counter = 0\n",
    "    for word in question.split():\n",
    "        if word in embeddings:\n",
    "            words_embedding_score += np.array(embeddings[word])\n",
    "            counter += 1\n",
    "    if counter != 0:\n",
    "        words_embedding_score /= counter\n",
    "    return words_embedding_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059027776\n",
      "-0.028727213541666668\n",
      "0.046056111653645836\n",
      "0.08525933159722222\n",
      "0.02430555555555...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    operation = np.array(dup_ranks) <= np.array([k])\n",
    "    return np.average(operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    operation = np.array(dup_ranks) <= np.array([k])\n",
    "    return np.average((1/np.log2(1+np.array(dup_ranks)))*operation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv') ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    question_vec_score = question_to_vec(question, embeddings, dim)\n",
    "    question_vec = np.full((1,dim), question_vec_score)\n",
    "    candidates_vec = np.array([question_to_vec(candidate, embeddings, dim) for candidate in candidates])\n",
    "    cosine_score = np.array(cosine_similarity(question_vec, candidates_vec)[0])\n",
    "    merged_candidate_list = list(zip(cosine_score, range(len(candidates)), candidates))\n",
    "    sorted_candidate_list  = sorted(merged_candidate_list, key=lambda x: x[0], reverse=True)\n",
    "    result = [(index, candidate) for consine_score, index, candidate in sorted_candidate_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(word) for word in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.310 | Hits@   1: 0.310\n",
      "DCG@   5: 0.380 | Hits@   5: 0.443\n",
      "DCG@  10: 0.397 | Hits@  10: 0.494\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.661\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/train.tsv', 'data/prepared_train.tsv')\n",
    "prepare_file('data/test.tsv', 'data/prepared_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/prepared_test.tsv' ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/prepared_train.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/prepared_train.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79.1%  lr: 0.042322  loss: 0.009911  eta: 0h13m  tot: 0h2m29s  (15.8%)%  lr: 0.049950  loss: 0.058007  eta: 0h15m  tot: 0h0m1s  (0.2%)3.1%  lr: 0.049750  loss: 0.035159  eta: 0h15m  tot: 0h0m6s  (0.6%)3.2%  lr: 0.049740  loss: 0.034745  eta: 0h15m  tot: 0h0m6s  (0.6%)5.2%  lr: 0.049510  loss: 0.028529  eta: 0h18m  tot: 0h0m11s  (1.0%)5.3%  lr: 0.049489  loss: 0.028202  eta: 0h18m  tot: 0h0m11s  (1.1%)5.6%  lr: 0.049459  loss: 0.027509  eta: 0h18m  tot: 0h0m12s  (1.1%)5.9%  lr: 0.049429  loss: 0.027010  eta: 0h18m  tot: 0h0m13s  (1.2%)%  lr: 0.049379  loss: 0.026249  eta: 0h18m  tot: 0h0m14s  (1.3%)6.7%  lr: 0.049359  loss: 0.025924  eta: 0h18m  tot: 0h0m14s  (1.3%)8.9%  lr: 0.049099  loss: 0.023236  eta: 0h17m  tot: 0h0m19s  (1.8%)12.2%  lr: 0.048779  loss: 0.020453  eta: 0h16m  tot: 0h0m25s  (2.4%)12.4%  lr: 0.048779  loss: 0.020277  eta: 0h16m  tot: 0h0m25s  (2.5%)12.7%  lr: 0.048749  loss: 0.020099  eta: 0h17m  tot: 0h0m26s  (2.5%)%  lr: 0.048619  loss: 0.019395  eta: 0h16m  tot: 0h0m28s  (2.8%)14.2%  lr: 0.048579  loss: 0.019231  eta: 0h16m  tot: 0h0m29s  (2.8%)14.3%  lr: 0.048579  loss: 0.019236  eta: 0h16m  tot: 0h0m29s  (2.9%)15.0%  lr: 0.048509  loss: 0.018907  eta: 0h16m  tot: 0h0m30s  (3.0%)15.5%  lr: 0.048448  loss: 0.018678  eta: 0h16m  tot: 0h0m31s  (3.1%)15.7%  lr: 0.048408  loss: 0.018613  eta: 0h16m  tot: 0h0m31s  (3.1%)%  lr: 0.048038  loss: 0.017044  eta: 0h16m  tot: 0h0m40s  (3.9%)20.0%  lr: 0.048028  loss: 0.016906  eta: 0h16m  tot: 0h0m41s  (4.0%)20.4%  lr: 0.047988  loss: 0.016763  eta: 0h16m  tot: 0h0m41s  (4.1%)20.7%  lr: 0.047928  loss: 0.016692  eta: 0h16m  tot: 0h0m42s  (4.1%)21.2%  lr: 0.047908  loss: 0.016506  eta: 0h16m  tot: 0h0m43s  (4.2%)22.9%  lr: 0.047738  loss: 0.016071  eta: 0h16m  tot: 0h0m46s  (4.6%)23.0%  lr: 0.047728  loss: 0.016039  eta: 0h16m  tot: 0h0m47s  (4.6%)23.9%  lr: 0.047628  loss: 0.015722  eta: 0h16m  tot: 0h0m49s  (4.8%)24.7%  lr: 0.047588  loss: 0.015540  eta: 0h16m  tot: 0h0m50s  (4.9%)24.9%  lr: 0.047548  loss: 0.015461  eta: 0h16m  tot: 0h0m51s  (5.0%)26.0%  lr: 0.047447  loss: 0.015193  eta: 0h16m  tot: 0h0m53s  (5.2%)26.3%  lr: 0.047407  loss: 0.015158  eta: 0h16m  tot: 0h0m54s  (5.3%)26.4%  lr: 0.047407  loss: 0.015137  eta: 0h16m  tot: 0h0m54s  (5.3%)26.9%  lr: 0.047377  loss: 0.015033  eta: 0h16m  tot: 0h0m55s  (5.4%)27.6%  lr: 0.047337  loss: 0.014827  eta: 0h16m  tot: 0h0m57s  (5.5%)28.3%  lr: 0.047237  loss: 0.014727  eta: 0h16m  tot: 0h0m58s  (5.7%)0.047057  loss: 0.014274  eta: 0h16m  tot: 0h1m3s  (6.1%)30.8%  lr: 0.047057  loss: 0.014272  eta: 0h16m  tot: 0h1m3s  (6.2%)31.1%  lr: 0.047047  loss: 0.014212  eta: 0h16m  tot: 0h1m3s  (6.2%)32.5%  lr: 0.046937  loss: 0.013927  eta: 0h15m  tot: 0h1m6s  (6.5%)33.4%  lr: 0.046897  loss: 0.013774  eta: 0h15m  tot: 0h1m8s  (6.7%)35.4%  lr: 0.046637  loss: 0.013461  eta: 0h15m  tot: 0h1m11s  (7.1%)35.6%  lr: 0.046627  loss: 0.013429  eta: 0h15m  tot: 0h1m12s  (7.1%)  lr: 0.046547  loss: 0.013346  eta: 0h15m  tot: 0h1m13s  (7.2%)37.3%  lr: 0.046447  loss: 0.013194  eta: 0h15m  tot: 0h1m15s  (7.5%)37.4%  lr: 0.046416  loss: 0.013191  eta: 0h15m  tot: 0h1m15s  (7.5%)38.1%  lr: 0.046316  loss: 0.013075  eta: 0h15m  tot: 0h1m16s  (7.6%)39.1%  lr: 0.046226  loss: 0.012973  eta: 0h15m  tot: 0h1m18s  (7.8%)39.9%  lr: 0.046176  loss: 0.012884  eta: 0h15m  tot: 0h1m19s  (8.0%)40.1%  lr: 0.046166  loss: 0.012860  eta: 0h15m  tot: 0h1m20s  (8.0%)40.5%  lr: 0.046126  loss: 0.012816  eta: 0h15m  tot: 0h1m20s  (8.1%)40.6%  lr: 0.046126  loss: 0.012799  eta: 0h15m  tot: 0h1m20s  (8.1%)42.0%  lr: 0.045966  loss: 0.012619  eta: 0h15m  tot: 0h1m23s  (8.4%)42.6%  lr: 0.045916  loss: 0.012556  eta: 0h15m  tot: 0h1m24s  (8.5%)42.8%  lr: 0.045866  loss: 0.012542  eta: 0h15m  tot: 0h1m24s  (8.6%)43.4%  lr: 0.045826  loss: 0.012471  eta: 0h15m  tot: 0h1m25s  (8.7%)44.1%  lr: 0.045746  loss: 0.012395  eta: 0h14m  tot: 0h1m27s  (8.8%)45.8%  lr: 0.045586  loss: 0.012198  eta: 0h14m  tot: 0h1m30s  (9.2%)45.9%  lr: 0.045586  loss: 0.012190  eta: 0h14m  tot: 0h1m30s  (9.2%)46.8%  lr: 0.045496  loss: 0.012121  eta: 0h14m  tot: 0h1m31s  (9.4%)47.2%  lr: 0.045456  loss: 0.012094  eta: 0h14m  tot: 0h1m32s  (9.4%)47.4%  lr: 0.045405  loss: 0.012067  eta: 0h14m  tot: 0h1m33s  (9.5%)47.6%  lr: 0.045365  loss: 0.012046  eta: 0h14m  tot: 0h1m33s  (9.5%)48.3%  lr: 0.045265  loss: 0.011992  eta: 0h14m  tot: 0h1m34s  (9.7%)49.1%  lr: 0.045185  loss: 0.011915  eta: 0h14m  tot: 0h1m35s  (9.8%)49.8%  lr: 0.045115  loss: 0.011838  eta: 0h14m  tot: 0h1m37s  (10.0%)49.9%  lr: 0.045115  loss: 0.011827  eta: 0h14m  tot: 0h1m37s  (10.0%)50.3%  lr: 0.045085  loss: 0.011787  eta: 0h14m  tot: 0h1m37s  (10.1%)51.9%  lr: 0.044945  loss: 0.011654  eta: 0h14m  tot: 0h1m40s  (10.4%)52.0%  lr: 0.044915  loss: 0.011629  eta: 0h14m  tot: 0h1m40s  (10.4%)  eta: 0h14m  tot: 0h1m41s  (10.5%)53.1%  lr: 0.044855  loss: 0.011572  eta: 0h14m  tot: 0h1m42s  (10.6%)54.4%  lr: 0.044715  loss: 0.011466  eta: 0h14m  tot: 0h1m45s  (10.9%)54.5%  lr: 0.044715  loss: 0.011456  eta: 0h14m  tot: 0h1m45s  (10.9%)55.1%  lr: 0.044655  loss: 0.011414  eta: 0h14m  tot: 0h1m46s  (11.0%)%)56.5%  lr: 0.044545  loss: 0.011335  eta: 0h14m  tot: 0h1m48s  (11.3%)57.5%  lr: 0.044425  loss: 0.011265  eta: 0h14m  tot: 0h1m50s  (11.5%)57.8%  lr: 0.044384  loss: 0.011244  eta: 0h14m  tot: 0h1m50s  (11.6%)58.2%  lr: 0.044344  loss: 0.011212  eta: 0h14m  tot: 0h1m51s  (11.6%)58.3%  lr: 0.044344  loss: 0.011205  eta: 0h14m  tot: 0h1m51s  (11.7%)58.5%  lr: 0.044334  loss: 0.011197  eta: 0h14m  tot: 0h1m51s  (11.7%)58.8%  lr: 0.044304  loss: 0.011170  eta: 0h14m  tot: 0h1m52s  (11.8%)59.5%  lr: 0.044244  loss: 0.011115  eta: 0h13m  tot: 0h1m53s  (11.9%)0h13m  tot: 0h1m55s  (12.2%)61.0%  lr: 0.044094  loss: 0.010998  eta: 0h13m  tot: 0h1m55s  (12.2%)61.8%  lr: 0.043934  loss: 0.010935  eta: 0h13m  tot: 0h1m57s  (12.4%)62.8%  lr: 0.043834  loss: 0.010852  eta: 0h13m  tot: 0h1m59s  (12.6%)63.6%  lr: 0.043744  loss: 0.010780  eta: 0h13m  tot: 0h2m0s  (12.7%)63.9%  lr: 0.043714  loss: 0.010764  eta: 0h13m  tot: 0h2m1s  (12.8%)65.4%  lr: 0.043604  loss: 0.010676  eta: 0h13m  tot: 0h2m3s  (13.1%)65.7%  lr: 0.043544  loss: 0.010655  eta: 0h13m  tot: 0h2m4s  (13.1%)66.4%  lr: 0.043454  loss: 0.010626  eta: 0h13m  tot: 0h2m5s  (13.3%)66.5%  lr: 0.043444  loss: 0.010617  eta: 0h13m  tot: 0h2m5s  (13.3%)66.7%  lr: 0.043424  loss: 0.010605  eta: 0h13m  tot: 0h2m6s  (13.3%)67.2%  lr: 0.043373  loss: 0.010569  eta: 0h13m  tot: 0h2m7s  (13.4%)67.5%  lr: 0.043363  loss: 0.010544  eta: 0h13m  tot: 0h2m7s  (13.5%)67.7%  lr: 0.043333  loss: 0.010530  eta: 0h13m  tot: 0h2m8s  (13.5%)68.8%  lr: 0.043183  loss: 0.010467  eta: 0h13m  tot: 0h2m10s  (13.8%)69.0%  lr: 0.043153  loss: 0.010458  eta: 0h13m  tot: 0h2m10s  (13.8%)69.3%  lr: 0.043133  loss: 0.010443  eta: 0h13m  tot: 0h2m11s  (13.9%)69.4%  lr: 0.043133  loss: 0.010433  eta: 0h13m  tot: 0h2m11s  (13.9%)69.9%  lr: 0.043133  loss: 0.010401  eta: 0h13m  tot: 0h2m12s  (14.0%)69.9%  lr: 0.043123  loss: 0.010400  eta: 0h13m  tot: 0h2m12s  (14.0%)70.1%  lr: 0.043113  loss: 0.010387  eta: 0h13m  tot: 0h2m13s  (14.0%)71.8%  lr: 0.042953  loss: 0.010298  eta: 0h13m  tot: 0h2m16s  (14.4%)72.5%  lr: 0.042923  loss: 0.010266  eta: 0h13m  tot: 0h2m17s  (14.5%)73.2%  lr: 0.042853  loss: 0.010228  eta: 0h13m  tot: 0h2m18s  (14.6%)73.3%  lr: 0.042823  loss: 0.010219  eta: 0h13m  tot: 0h2m18s  (14.7%)73.7%  lr: 0.042803  loss: 0.010190  eta: 0h13m  tot: 0h2m19s  (14.7%)74.4%  lr: 0.042713  loss: 0.010152  eta: 0h13m  tot: 0h2m20s  (14.9%)74.9%  lr: 0.042633  loss: 0.010129  eta: 0h13m  tot: 0h2m21s  (15.0%)75.6%  lr: 0.042613  loss: 0.010090  eta: 0h13m  tot: 0h2m22s  (15.1%)75.8%  lr: 0.042603  loss: 0.010078  eta: 0h13m  tot: 0h2m23s  (15.2%)76.2%  lr: 0.042593  loss: 0.010057  eta: 0h13m  tot: 0h2m23s  (15.2%)76.6%  lr: 0.042573  loss: 0.010031  eta: 0h13m  tot: 0h2m24s  (15.3%)76.7%  lr: 0.042553  loss: 0.010023  eta: 0h13m  tot: 0h2m24s  (15.3%)77.1%  lr: 0.042523  loss: 0.009998  eta: 0h13m  tot: 0h2m25s  (15.4%)77.4%  lr: 0.042523  loss: 0.009990  eta: 0h13m  tot: 0h2m25s  (15.5%)78.9%  lr: 0.042372  loss: 0.009922  eta: 0h13m  tot: 0h2m28s  (15.8%)79.2%  lr: 0.042302  loss: 0.009904  eta: 0h13m  tot: 0h2m29s  (15.8%)Epoch: 100.0%  lr: 0.040010  loss: 0.009034  eta: 0h12m  tot: 0h3m5s  (20.0%)79.6%  lr: 0.042252  loss: 0.009897  eta: 0h13m  tot: 0h2m29s  (15.9%)80.0%  lr: 0.042222  loss: 0.009873  eta: 0h13m  tot: 0h2m30s  (16.0%)80.2%  lr: 0.042222  loss: 0.009866  eta: 0h13m  tot: 0h2m30s  (16.0%)80.7%  lr: 0.042182  loss: 0.009846  eta: 0h13m  tot: 0h2m31s  (16.1%)81.3%  lr: 0.042092  loss: 0.009825  eta: 0h13m  tot: 0h2m32s  (16.3%)  eta: 0h13m  tot: 0h2m36s  (16.6%)83.4%  lr: 0.041872  loss: 0.009733  eta: 0h13m  tot: 0h2m36s  (16.7%)85.9%  lr: 0.041502  loss: 0.009606  eta: 0h12m  tot: 0h2m41s  (17.2%)86.0%  lr: 0.041482  loss: 0.009603  eta: 0h12m  tot: 0h2m41s  (17.2%)%  lr: 0.041482  loss: 0.009598  eta: 0h12m  tot: 0h2m41s  (17.2%)86.2%  lr: 0.041462  loss: 0.009594  eta: 0h12m  tot: 0h2m41s  (17.2%)86.6%  lr: 0.041412  loss: 0.009573  eta: 0h12m  tot: 0h2m42s  (17.3%)88.2%  lr: 0.041211  loss: 0.009500  eta: 0h12m  tot: 0h2m45s  (17.6%)88.6%  lr: 0.041201  loss: 0.009494  eta: 0h12m  tot: 0h2m45s  (17.7%)89.4%  lr: 0.041051  loss: 0.009458  eta: 0h12m  tot: 0h2m47s  (17.9%)90.0%  lr: 0.041031  loss: 0.009429  eta: 0h12m  tot: 0h2m48s  (18.0%)90.2%  lr: 0.040971  loss: 0.009426  eta: 0h12m  tot: 0h2m48s  (18.0%)90.5%  lr: 0.040901  loss: 0.009410  eta: 0h12m  tot: 0h2m49s  (18.1%)90.6%  lr: 0.040891  loss: 0.009409  eta: 0h12m  tot: 0h2m49s  (18.1%)92.5%  lr: 0.040751  loss: 0.009336  eta: 0h12m  tot: 0h2m52s  (18.5%)93.4%  lr: 0.040621  loss: 0.009299  eta: 0h12m  tot: 0h2m53s  (18.7%)94.0%  lr: 0.040541  loss: 0.009283  eta: 0h12m  tot: 0h2m54s  (18.8%)94.1%  lr: 0.040541  loss: 0.009280  eta: 0h12m  tot: 0h2m55s  (18.8%)95.2%  lr: 0.040461  loss: 0.009230  eta: 0h12m  tot: 0h2m57s  (19.0%)95.6%  lr: 0.040401  loss: 0.009217  eta: 0h12m  tot: 0h2m57s  (19.1%)95.7%  lr: 0.040401  loss: 0.009211  eta: 0h12m  tot: 0h2m57s  (19.1%)97.0%  lr: 0.040290  loss: 0.009162  eta: 0h12m  tot: 0h3m0s  (19.4%)97.4%  lr: 0.040240  loss: 0.009144  eta: 0h12m  tot: 0h3m0s  (19.5%)97.6%  lr: 0.040210  loss: 0.009137  eta: 0h12m  tot: 0h3m1s  (19.5%)98.0%  lr: 0.040170  loss: 0.009123  eta: 0h12m  tot: 0h3m1s  (19.6%)98.1%  lr: 0.040170  loss: 0.009116  eta: 0h12m  tot: 0h3m2s  (19.6%)98.4%  lr: 0.040150  loss: 0.009101  eta: 0h12m  tot: 0h3m2s  (19.7%)99.1%  lr: 0.040110  loss: 0.009070  eta: 0h12m  tot: 0h3m3s  (19.8%)99.3%  lr: 0.040110  loss: 0.009067  eta: 0h12m  tot: 0h3m3s  (19.9%)\n",
      " ---+++                Epoch    0 Train error : 0.00898929 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73.5%  lr: 0.032573  loss: 0.002698  eta: 0h9m  tot: 0h5m18s  (34.7%))%  lr: 0.039990  loss: 0.003533  eta: 0h4m  tot: 0h3m6s  (20.1%)0.8%  lr: 0.039950  loss: 0.003207  eta: 0h7m  tot: 0h3m6s  (20.2%)1.7%  lr: 0.039840  loss: 0.003331  eta: 0h9m  tot: 0h3m8s  (20.3%)2.1%  lr: 0.039780  loss: 0.003079  eta: 0h9m  tot: 0h3m9s  (20.4%)2.6%  lr: 0.039760  loss: 0.002947  eta: 0h9m  tot: 0h3m9s  (20.5%)3.6%  lr: 0.039660  loss: 0.002992  eta: 0h9m  tot: 0h3m11s  (20.7%)3.7%  lr: 0.039650  loss: 0.002961  eta: 0h10m  tot: 0h3m11s  (20.7%)%  lr: 0.039620  loss: 0.002974  eta: 0h10m  tot: 0h3m12s  (20.8%)4.3%  lr: 0.039600  loss: 0.002963  eta: 0h10m  tot: 0h3m12s  (20.9%)6.1%  lr: 0.039389  loss: 0.002755  eta: 0h10m  tot: 0h3m15s  (21.2%)7.6%  lr: 0.039269  loss: 0.002724  eta: 0h10m  tot: 0h3m18s  (21.5%)7.9%  lr: 0.039219  loss: 0.002784  eta: 0h10m  tot: 0h3m18s  (21.6%)9.0%  lr: 0.039129  loss: 0.002813  eta: 0h10m  tot: 0h3m20s  (21.8%)10.1%  lr: 0.039009  loss: 0.002845  eta: 0h10m  tot: 0h3m22s  (22.0%)11.0%  lr: 0.038919  loss: 0.002846  eta: 0h10m  tot: 0h3m24s  (22.2%)12.0%  lr: 0.038809  loss: 0.002872  eta: 0h10m  tot: 0h3m25s  (22.4%)12.4%  lr: 0.038769  loss: 0.002864  eta: 0h10m  tot: 0h3m26s  (22.5%)12.5%  lr: 0.038769  loss: 0.002854  eta: 0h10m  tot: 0h3m26s  (22.5%)12.8%  lr: 0.038769  loss: 0.002843  eta: 0h10m  tot: 0h3m27s  (22.6%)13.2%  lr: 0.038689  loss: 0.002825  eta: 0h10m  tot: 0h3m28s  (22.6%)14.0%  lr: 0.038599  loss: 0.002780  eta: 0h10m  tot: 0h3m29s  (22.8%)14.1%  lr: 0.038599  loss: 0.002781  eta: 0h10m  tot: 0h3m29s  (22.8%)14.2%  lr: 0.038579  loss: 0.002781  eta: 0h10m  tot: 0h3m29s  (22.8%)%  lr: 0.038448  loss: 0.002774  eta: 0h10m  tot: 0h3m31s  (23.0%)15.7%  lr: 0.038378  loss: 0.002750  eta: 0h10m  tot: 0h3m32s  (23.1%)15.8%  lr: 0.038378  loss: 0.002744  eta: 0h10m  tot: 0h3m32s  (23.2%)16.1%  lr: 0.038348  loss: 0.002735  eta: 0h10m  tot: 0h3m33s  (23.2%)16.2%  lr: 0.038328  loss: 0.002743  eta: 0h10m  tot: 0h3m33s  (23.2%)%  lr: 0.038298  loss: 0.002741  eta: 0h10m  tot: 0h3m34s  (23.3%)17.6%  lr: 0.038148  loss: 0.002765  eta: 0h10m  tot: 0h3m36s  (23.5%)17.8%  lr: 0.038138  loss: 0.002772  eta: 0h10m  tot: 0h3m36s  (23.6%)18.2%  lr: 0.038098  loss: 0.002777  eta: 0h10m  tot: 0h3m37s  (23.6%)18.3%  lr: 0.038088  loss: 0.002769  eta: 0h10m  tot: 0h3m37s  (23.7%)18.5%  lr: 0.038048  loss: 0.002779  eta: 0h10m  tot: 0h3m37s  (23.7%)20.3%  lr: 0.037908  loss: 0.002745  eta: 0h10m  tot: 0h3m40s  (24.1%)21.2%  lr: 0.037828  loss: 0.002772  eta: 0h10m  tot: 0h3m42s  (24.2%)21.5%  lr: 0.037818  loss: 0.002767  eta: 0h10m  tot: 0h3m42s  (24.3%)22.2%  lr: 0.037718  loss: 0.002775  eta: 0h10m  tot: 0h3m44s  (24.4%)22.6%  lr: 0.037698  loss: 0.002766  eta: 0h10m  tot: 0h3m44s  (24.5%)23.0%  lr: 0.037638  loss: 0.002763  eta: 0h10m  tot: 0h3m45s  (24.6%)25.2%  lr: 0.037457  loss: 0.002774  eta: 0h10m  tot: 0h3m48s  (25.0%)25.7%  lr: 0.037377  loss: 0.002775  eta: 0h10m  tot: 0h3m49s  (25.1%)26.1%  lr: 0.037317  loss: 0.002791  eta: 0h10m  tot: 0h3m50s  (25.2%)26.3%  lr: 0.037307  loss: 0.002794  eta: 0h10m  tot: 0h3m50s  (25.3%)26.5%  lr: 0.037257  loss: 0.002789  eta: 0h10m  tot: 0h3m51s  (25.3%)28.4%  lr: 0.037037  loss: 0.002779  eta: 0h10m  tot: 0h3m54s  (25.7%)28.7%  lr: 0.037017  loss: 0.002770  eta: 0h10m  tot: 0h3m55s  (25.7%)28.8%  lr: 0.036997  loss: 0.002769  eta: 0h10m  tot: 0h3m55s  (25.8%)29.3%  lr: 0.036917  loss: 0.002762  eta: 0h10m  tot: 0h3m56s  (25.9%)29.4%  lr: 0.036907  loss: 0.002757  eta: 0h10m  tot: 0h3m56s  (25.9%)29.7%  lr: 0.036867  loss: 0.002764  eta: 0h10m  tot: 0h3m57s  (25.9%)30.0%  lr: 0.036847  loss: 0.002755  eta: 0h10m  tot: 0h3m57s  (26.0%)30.1%  lr: 0.036847  loss: 0.002750  eta: 0h10m  tot: 0h3m57s  (26.0%)%  lr: 0.036607  loss: 0.002757  eta: 0h10m  tot: 0h4m1s  (26.4%)32.5%  lr: 0.036567  loss: 0.002766  eta: 0h10m  tot: 0h4m2s  (26.5%)32.9%  lr: 0.036517  loss: 0.002756  eta: 0h10m  tot: 0h4m3s  (26.6%)33.4%  lr: 0.036447  loss: 0.002757  eta: 0h10m  tot: 0h4m4s  (26.7%)33.6%  lr: 0.036416  loss: 0.002751  eta: 0h10m  tot: 0h4m5s  (26.7%)34.0%  lr: 0.036376  loss: 0.002747  eta: 0h10m  tot: 0h4m5s  (26.8%)34.1%  lr: 0.036366  loss: 0.002746  eta: 0h10m  tot: 0h4m6s  (26.8%)34.2%  lr: 0.036356  loss: 0.002747  eta: 0h10m  tot: 0h4m6s  (26.8%)34.7%  lr: 0.036296  loss: 0.002737  eta: 0h10m  tot: 0h4m7s  (26.9%)%  lr: 0.036256  loss: 0.002728  eta: 0h10m  tot: 0h4m7s  (27.0%)36.5%  lr: 0.036136  loss: 0.002729  eta: 0h10m  tot: 0h4m11s  (27.3%)37.4%  lr: 0.036026  loss: 0.002724  eta: 0h10m  tot: 0h4m12s  (27.5%)37.6%  lr: 0.035996  loss: 0.002721  eta: 0h10m  tot: 0h4m13s  (27.5%)39.0%  lr: 0.035876  loss: 0.002720  eta: 0h10m  tot: 0h4m16s  (27.8%)39.3%  lr: 0.035806  loss: 0.002721  eta: 0h10m  tot: 0h4m17s  (27.9%)  lr: 0.035806  loss: 0.002720  eta: 0h10m  tot: 0h4m17s  (27.9%)39.9%  lr: 0.035776  loss: 0.002730  eta: 0h10m  tot: 0h4m18s  (28.0%)40.1%  lr: 0.035746  loss: 0.002735  eta: 0h10m  tot: 0h4m18s  (28.0%)%  lr: 0.035726  loss: 0.002731  eta: 0h10m  tot: 0h4m19s  (28.0%)44.5%  lr: 0.035355  loss: 0.002754  eta: 0h10m  tot: 0h4m26s  (28.9%)29.1%)45.5%  lr: 0.035225  loss: 0.002758  eta: 0h10m  tot: 0h4m28s  (29.1%)46.0%  lr: 0.035215  loss: 0.002762  eta: 0h10m  tot: 0h4m29s  (29.2%)%  lr: 0.035185  loss: 0.002763  eta: 0h10m  tot: 0h4m30s  (29.3%)%  lr: 0.035135  loss: 0.002760  eta: 0h10m  tot: 0h4m30s  (29.4%)47.2%  lr: 0.035095  loss: 0.002764  eta: 0h10m  tot: 0h4m31s  (29.4%)47.3%  lr: 0.035075  loss: 0.002766  eta: 0h10m  tot: 0h4m31s  (29.5%)47.8%  lr: 0.035025  loss: 0.002764  eta: 0h10m  tot: 0h4m32s  (29.6%)48.1%  lr: 0.035025  loss: 0.002758  eta: 0h10m  tot: 0h4m32s  (29.6%)%  lr: 0.035015  loss: 0.002759  eta: 0h10m  tot: 0h4m33s  (29.6%)  loss: 0.002765  eta: 0h10m  tot: 0h4m34s  (29.8%)50.4%  lr: 0.034825  loss: 0.002749  eta: 0h10m  tot: 0h4m37s  (30.1%)52.1%  lr: 0.034615  loss: 0.002740  eta: 0h10m  tot: 0h4m39s  (30.4%)53.1%  lr: 0.034525  loss: 0.002738  eta: 0h10m  tot: 0h4m41s  (30.6%)53.5%  lr: 0.034465  loss: 0.002740  eta: 0h10m  tot: 0h4m41s  (30.7%)30.9%)54.9%  lr: 0.034344  loss: 0.002744  eta: 0h10m  tot: 0h4m45s  (31.0%)55.1%  lr: 0.034314  loss: 0.002744  eta: 0h10m  tot: 0h4m45s  (31.0%)55.5%  lr: 0.034234  loss: 0.002744  eta: 0h10m  tot: 0h4m46s  (31.1%)56.1%  lr: 0.034204  loss: 0.002732  eta: 0h10m  tot: 0h4m47s  (31.2%)56.5%  lr: 0.034184  loss: 0.002729  eta: 0h10m  tot: 0h4m47s  (31.3%)58.8%  lr: 0.033984  loss: 0.002741  eta: 0h10m  tot: 0h4m51s  (31.8%)59.9%  lr: 0.033874  loss: 0.002735  eta: 0h10m  tot: 0h4m54s  (32.0%)60.2%  lr: 0.033864  loss: 0.002743  eta: 0h10m  tot: 0h4m54s  (32.0%)62.1%  lr: 0.033684  loss: 0.002736  eta: 0h10m  tot: 0h4m58s  (32.4%)62.3%  lr: 0.033674  loss: 0.002739  eta: 0h10m  tot: 0h4m58s  (32.5%)62.8%  lr: 0.033574  loss: 0.002739  eta: 0h10m  tot: 0h4m59s  (32.6%)63.4%  lr: 0.033504  loss: 0.002741  eta: 0h10m  tot: 0h5m0s  (32.7%)64.8%  lr: 0.033363  loss: 0.002736  eta: 0h10m  tot: 0h5m3s  (33.0%)65.0%  lr: 0.033343  loss: 0.002733  eta: 0h10m  tot: 0h5m3s  (33.0%)65.4%  lr: 0.033333  loss: 0.002730  eta: 0h10m  tot: 0h5m4s  (33.1%)66.0%  lr: 0.033233  loss: 0.002722  eta: 0h10m  tot: 0h5m5s  (33.2%)66.2%  lr: 0.033193  loss: 0.002725  eta: 0h10m  tot: 0h5m5s  (33.2%)67.0%  lr: 0.033133  loss: 0.002725  eta: 0h10m  tot: 0h5m6s  (33.4%)67.4%  lr: 0.033093  loss: 0.002717  eta: 0h9m  tot: 0h5m7s  (33.5%)69.6%  lr: 0.032943  loss: 0.002714  eta: 0h9m  tot: 0h5m11s  (33.9%)69.7%  lr: 0.032943  loss: 0.002715  eta: 0h9m  tot: 0h5m11s  (33.9%)70.8%  lr: 0.032833  loss: 0.002707  eta: 0h9m  tot: 0h5m13s  (34.2%)71.0%  lr: 0.032823  loss: 0.002707  eta: 0h9m  tot: 0h5m13s  (34.2%)71.4%  lr: 0.032803  loss: 0.002708  eta: 0h9m  tot: 0h5m14s  (34.3%)71.8%  lr: 0.032773  loss: 0.002705  eta: 0h9m  tot: 0h5m15s  (34.4%)72.1%  lr: 0.032743  loss: 0.002704  eta: 0h9m  tot: 0h5m15s  (34.4%)72.4%  lr: 0.032743  loss: 0.002700  eta: 0h9m  tot: 0h5m16s  (34.5%)72.5%  lr: 0.032743  loss: 0.002700  eta: 0h9m  tot: 0h5m16s  (34.5%)72.6%  lr: 0.032713  loss: 0.002700  eta: 0h9m  tot: 0h5m16s  (34.5%)0.032613  loss: 0.002699  eta: 0h9m  tot: 0h5m17s  (34.7%)73.6%  lr: 0.032573  loss: 0.002699  eta: 0h9m  tot: 0h5m18s  (34.7%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002658  eta: 0h8m  tot: 0h6m3s  (40.0%)73.9%  lr: 0.032563  loss: 0.002698  eta: 0h9m  tot: 0h5m18s  (34.8%)74.0%  lr: 0.032553  loss: 0.002697  eta: 0h9m  tot: 0h5m19s  (34.8%)74.6%  lr: 0.032503  loss: 0.002701  eta: 0h9m  tot: 0h5m20s  (34.9%)74.9%  lr: 0.032483  loss: 0.002701  eta: 0h9m  tot: 0h5m20s  (35.0%)75.2%  lr: 0.032453  loss: 0.002697  eta: 0h9m  tot: 0h5m21s  (35.0%)75.7%  lr: 0.032413  loss: 0.002704  eta: 0h9m  tot: 0h5m22s  (35.1%)75.8%  lr: 0.032413  loss: 0.002704  eta: 0h9m  tot: 0h5m22s  (35.2%)76.2%  lr: 0.032362  loss: 0.002704  eta: 0h9m  tot: 0h5m23s  (35.2%)%  lr: 0.032212  loss: 0.002703  eta: 0h9m  tot: 0h5m25s  (35.5%)h5m26s  (35.6%)78.3%  lr: 0.032102  loss: 0.002706  eta: 0h9m  tot: 0h5m26s  (35.7%)78.4%  lr: 0.032082  loss: 0.002706  eta: 0h9m  tot: 0h5m26s  (35.7%)79.3%  lr: 0.032002  loss: 0.002712  eta: 0h9m  tot: 0h5m29s  (35.9%)80.4%  lr: 0.031932  loss: 0.002710  eta: 0h9m  tot: 0h5m30s  (36.1%)80.7%  lr: 0.031912  loss: 0.002711  eta: 0h9m  tot: 0h5m31s  (36.1%)80.8%  lr: 0.031882  loss: 0.002711  eta: 0h9m  tot: 0h5m31s  (36.2%)80.9%  lr: 0.031852  loss: 0.002711  eta: 0h9m  tot: 0h5m31s  (36.2%)84.1%  lr: 0.031532  loss: 0.002709  eta: 0h9m  tot: 0h5m37s  (36.8%)85.0%  lr: 0.031432  loss: 0.002699  eta: 0h9m  tot: 0h5m38s  (37.0%)85.6%  lr: 0.031351  loss: 0.002697  eta: 0h9m  tot: 0h5m40s  (37.1%)5m41s  (37.3%)86.5%  lr: 0.031291  loss: 0.002693  eta: 0h9m  tot: 0h5m41s  (37.3%)86.7%  lr: 0.031251  loss: 0.002691  eta: 0h9m  tot: 0h5m41s  (37.3%)87.0%  lr: 0.031221  loss: 0.002688  eta: 0h9m  tot: 0h5m42s  (37.4%)87.3%  lr: 0.031151  loss: 0.002686  eta: 0h9m  tot: 0h5m42s  (37.5%)89.2%  lr: 0.031011  loss: 0.002676  eta: 0h9m  tot: 0h5m46s  (37.8%)89.7%  lr: 0.030931  loss: 0.002673  eta: 0h9m  tot: 0h5m46s  (37.9%)90.2%  lr: 0.030881  loss: 0.002674  eta: 0h9m  tot: 0h5m47s  (38.0%)91.0%  lr: 0.030821  loss: 0.002675  eta: 0h9m  tot: 0h5m49s  (38.2%)92.2%  lr: 0.030691  loss: 0.002675  eta: 0h9m  tot: 0h5m51s  (38.4%)93.1%  lr: 0.030591  loss: 0.002671  eta: 0h9m  tot: 0h5m52s  (38.6%)93.5%  lr: 0.030541  loss: 0.002671  eta: 0h9m  tot: 0h5m53s  (38.7%)95.1%  lr: 0.030421  loss: 0.002666  eta: 0h9m  tot: 0h5m56s  (39.0%)95.7%  lr: 0.030350  loss: 0.002667  eta: 0h9m  tot: 0h5m57s  (39.1%)96.1%  lr: 0.030320  loss: 0.002666  eta: 0h9m  tot: 0h5m58s  (39.2%)96.4%  lr: 0.030290  loss: 0.002665  eta: 0h9m  tot: 0h5m58s  (39.3%)97.1%  lr: 0.030210  loss: 0.002658  eta: 0h9m  tot: 0h6m0s  (39.4%)97.7%  lr: 0.030190  loss: 0.002661  eta: 0h9m  tot: 0h6m1s  (39.5%)\n",
      " ---+++                Epoch    1 Train error : 0.00267265 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80.7%  lr: 0.021832  loss: 0.001875  eta: 0h6m  tot: 0h8m24s  (56.1%)5%  lr: 0.029980  loss: 0.002045  eta: 0h6m  tot: 0h6m4s  (40.1%)1.9%  lr: 0.029820  loss: 0.002022  eta: 0h8m  tot: 0h6m7s  (40.4%)2.3%  lr: 0.029820  loss: 0.001930  eta: 0h8m  tot: 0h6m8s  (40.5%)3.1%  lr: 0.029760  loss: 0.001946  eta: 0h8m  tot: 0h6m9s  (40.6%)4.2%  lr: 0.029660  loss: 0.001935  eta: 0h9m  tot: 0h6m11s  (40.8%)4.8%  lr: 0.029630  loss: 0.001863  eta: 0h9m  tot: 0h6m12s  (41.0%)5.4%  lr: 0.029550  loss: 0.001895  eta: 0h8m  tot: 0h6m13s  (41.1%)5.6%  lr: 0.029540  loss: 0.001908  eta: 0h8m  tot: 0h6m14s  (41.1%)6.0%  lr: 0.029500  loss: 0.001880  eta: 0h8m  tot: 0h6m15s  (41.2%)6.3%  lr: 0.029489  loss: 0.001855  eta: 0h8m  tot: 0h6m15s  (41.3%)7.7%  lr: 0.029289  loss: 0.001812  eta: 0h8m  tot: 0h6m17s  (41.5%)9.2%  lr: 0.029119  loss: 0.001796  eta: 0h8m  tot: 0h6m20s  (41.8%)10.8%  lr: 0.028939  loss: 0.001827  eta: 0h8m  tot: 0h6m22s  (42.2%)12.1%  lr: 0.028759  loss: 0.001781  eta: 0h8m  tot: 0h6m24s  (42.4%)12.7%  lr: 0.028609  loss: 0.001790  eta: 0h8m  tot: 0h6m26s  (42.5%)%  lr: 0.028599  loss: 0.001788  eta: 0h8m  tot: 0h6m26s  (42.6%)13.2%  lr: 0.028569  loss: 0.001795  eta: 0h8m  tot: 0h6m27s  (42.6%)13.5%  lr: 0.028559  loss: 0.001794  eta: 0h8m  tot: 0h6m27s  (42.7%)13.6%  lr: 0.028559  loss: 0.001795  eta: 0h8m  tot: 0h6m27s  (42.7%)14.1%  lr: 0.028509  loss: 0.001813  eta: 0h8m  tot: 0h6m28s  (42.8%)%  lr: 0.028458  loss: 0.001826  eta: 0h8m  tot: 0h6m29s  (42.9%)%  lr: 0.028388  loss: 0.001840  eta: 0h8m  tot: 0h6m30s  (43.0%)%  lr: 0.028368  loss: 0.001835  eta: 0h8m  tot: 0h6m30s  (43.0%)%  lr: 0.028318  loss: 0.001843  eta: 0h8m  tot: 0h6m30s  (43.0%)15.8%  lr: 0.028218  loss: 0.001843  eta: 0h8m  tot: 0h6m31s  (43.2%)17.1%  lr: 0.028128  loss: 0.001859  eta: 0h8m  tot: 0h6m34s  (43.4%)17.6%  lr: 0.028098  loss: 0.001853  eta: 0h8m  tot: 0h6m35s  (43.5%)18.0%  lr: 0.028048  loss: 0.001843  eta: 0h8m  tot: 0h6m35s  (43.6%)19.1%  lr: 0.027838  loss: 0.001868  eta: 0h8m  tot: 0h6m38s  (43.8%)19.9%  lr: 0.027668  loss: 0.001864  eta: 0h8m  tot: 0h6m39s  (44.0%)20.2%  lr: 0.027668  loss: 0.001867  eta: 0h8m  tot: 0h6m39s  (44.0%)0.001861  eta: 0h8m  tot: 0h6m40s  (44.1%)21.2%  lr: 0.027578  loss: 0.001860  eta: 0h8m  tot: 0h6m42s  (44.2%)21.8%  lr: 0.027508  loss: 0.001863  eta: 0h8m  tot: 0h6m43s  (44.4%)23.5%  lr: 0.027387  loss: 0.001862  eta: 0h8m  tot: 0h6m46s  (44.7%)23.6%  lr: 0.027377  loss: 0.001859  eta: 0h8m  tot: 0h6m46s  (44.7%)23.7%  lr: 0.027357  loss: 0.001861  eta: 0h8m  tot: 0h6m47s  (44.7%)24.1%  lr: 0.027317  loss: 0.001860  eta: 0h8m  tot: 0h6m47s  (44.8%)24.8%  lr: 0.027267  loss: 0.001850  eta: 0h8m  tot: 0h6m48s  (45.0%)25.2%  lr: 0.027227  loss: 0.001855  eta: 0h8m  tot: 0h6m49s  (45.0%)26.3%  lr: 0.027087  loss: 0.001870  eta: 0h8m  tot: 0h6m51s  (45.3%)26.6%  lr: 0.027017  loss: 0.001863  eta: 0h8m  tot: 0h6m52s  (45.3%)27.2%  lr: 0.026987  loss: 0.001861  eta: 0h8m  tot: 0h6m53s  (45.4%)%  lr: 0.026987  loss: 0.001858  eta: 0h8m  tot: 0h6m53s  (45.5%)%  lr: 0.026977  loss: 0.001855  eta: 0h8m  tot: 0h6m53s  (45.5%)28.4%  lr: 0.026917  loss: 0.001857  eta: 0h8m  tot: 0h6m55s  (45.7%)28.5%  lr: 0.026907  loss: 0.001853  eta: 0h8m  tot: 0h6m55s  (45.7%)29.3%  lr: 0.026857  loss: 0.001850  eta: 0h8m  tot: 0h6m57s  (45.9%)%  lr: 0.026827  loss: 0.001838  eta: 0h8m  tot: 0h6m58s  (46.0%)31.1%  lr: 0.026667  loss: 0.001834  eta: 0h8m  tot: 0h7m0s  (46.2%)31.2%  lr: 0.026657  loss: 0.001835  eta: 0h8m  tot: 0h7m0s  (46.2%)32.1%  lr: 0.026567  loss: 0.001828  eta: 0h8m  tot: 0h7m1s  (46.4%)32.9%  lr: 0.026487  loss: 0.001840  eta: 0h8m  tot: 0h7m3s  (46.6%)33.2%  lr: 0.026457  loss: 0.001847  eta: 0h8m  tot: 0h7m3s  (46.6%)34.5%  lr: 0.026376  loss: 0.001855  eta: 0h7m  tot: 0h7m6s  (46.9%)35.2%  lr: 0.026276  loss: 0.001857  eta: 0h7m  tot: 0h7m7s  (47.0%)35.5%  lr: 0.026246  loss: 0.001858  eta: 0h7m  tot: 0h7m7s  (47.1%)35.9%  lr: 0.026206  loss: 0.001850  eta: 0h7m  tot: 0h7m8s  (47.2%)36.5%  lr: 0.026186  loss: 0.001854  eta: 0h7m  tot: 0h7m9s  (47.3%)38.3%  lr: 0.026056  loss: 0.001856  eta: 0h7m  tot: 0h7m12s  (47.7%)38.4%  lr: 0.026056  loss: 0.001857  eta: 0h7m  tot: 0h7m13s  (47.7%)39.6%  lr: 0.025896  loss: 0.001861  eta: 0h7m  tot: 0h7m15s  (47.9%)39.7%  lr: 0.025866  loss: 0.001861  eta: 0h7m  tot: 0h7m15s  (47.9%)40.6%  lr: 0.025796  loss: 0.001868  eta: 0h7m  tot: 0h7m17s  (48.1%)41.3%  lr: 0.025706  loss: 0.001873  eta: 0h7m  tot: 0h7m18s  (48.3%)42.0%  lr: 0.025676  loss: 0.001869  eta: 0h7m  tot: 0h7m19s  (48.4%)42.6%  lr: 0.025646  loss: 0.001875  eta: 0h7m  tot: 0h7m20s  (48.5%)42.7%  lr: 0.025646  loss: 0.001874  eta: 0h7m  tot: 0h7m20s  (48.5%)44.0%  lr: 0.025526  loss: 0.001864  eta: 0h7m  tot: 0h7m22s  (48.8%)44.2%  lr: 0.025516  loss: 0.001864  eta: 0h7m  tot: 0h7m22s  (48.8%)44.3%  lr: 0.025516  loss: 0.001863  eta: 0h7m  tot: 0h7m23s  (48.9%)44.5%  lr: 0.025496  loss: 0.001861  eta: 0h7m  tot: 0h7m23s  (48.9%)45.6%  lr: 0.025415  loss: 0.001861  eta: 0h7m  tot: 0h7m25s  (49.1%)46.1%  lr: 0.025375  loss: 0.001859  eta: 0h7m  tot: 0h7m25s  (49.2%)46.2%  lr: 0.025365  loss: 0.001868  eta: 0h7m  tot: 0h7m26s  (49.2%)47.3%  lr: 0.025225  loss: 0.001858  eta: 0h7m  tot: 0h7m28s  (49.5%)47.4%  lr: 0.025205  loss: 0.001857  eta: 0h7m  tot: 0h7m28s  (49.5%)47.8%  lr: 0.025125  loss: 0.001855  eta: 0h7m  tot: 0h7m28s  (49.6%)48.6%  lr: 0.025005  loss: 0.001859  eta: 0h7m  tot: 0h7m30s  (49.7%)49.2%  lr: 0.024945  loss: 0.001858  eta: 0h7m  tot: 0h7m31s  (49.8%)49.5%  lr: 0.024935  loss: 0.001857  eta: 0h7m  tot: 0h7m31s  (49.9%)50.6%  lr: 0.024795  loss: 0.001866  eta: 0h7m  tot: 0h7m33s  (50.1%)51.0%  lr: 0.024765  loss: 0.001865  eta: 0h7m  tot: 0h7m34s  (50.2%)51.6%  lr: 0.024685  loss: 0.001870  eta: 0h7m  tot: 0h7m35s  (50.3%)52.0%  lr: 0.024655  loss: 0.001873  eta: 0h7m  tot: 0h7m35s  (50.4%)54.2%  lr: 0.024475  loss: 0.001860  eta: 0h7m  tot: 0h7m40s  (50.8%)55.0%  lr: 0.024425  loss: 0.001860  eta: 0h7m  tot: 0h7m41s  (51.0%)56.4%  lr: 0.024274  loss: 0.001873  eta: 0h7m  tot: 0h7m44s  (51.3%)56.5%  lr: 0.024274  loss: 0.001877  eta: 0h7m  tot: 0h7m44s  (51.3%)57.3%  lr: 0.024194  loss: 0.001880  eta: 0h7m  tot: 0h7m45s  (51.5%)57.5%  lr: 0.024164  loss: 0.001880  eta: 0h7m  tot: 0h7m46s  (51.5%)57.6%  lr: 0.024164  loss: 0.001878  eta: 0h7m  tot: 0h7m46s  (51.5%)58.8%  lr: 0.024054  loss: 0.001883  eta: 0h7m  tot: 0h7m48s  (51.8%)60.2%  lr: 0.023914  loss: 0.001877  eta: 0h7m  tot: 0h7m50s  (52.0%)60.5%  lr: 0.023894  loss: 0.001875  eta: 0h7m  tot: 0h7m51s  (52.1%)%  lr: 0.023764  loss: 0.001875  eta: 0h7m  tot: 0h7m53s  (52.4%)h6m  tot: 0h7m56s  (52.8%)66.0%  lr: 0.023404  loss: 0.001884  eta: 0h6m  tot: 0h7m59s  (53.2%)%  lr: 0.023393  loss: 0.001884  eta: 0h6m  tot: 0h8m0s  (53.2%)%  lr: 0.023223  loss: 0.001883  eta: 0h6m  tot: 0h8m3s  (53.6%)68.2%  lr: 0.023193  loss: 0.001887  eta: 0h6m  tot: 0h8m3s  (53.6%)68.4%  lr: 0.023193  loss: 0.001885  eta: 0h6m  tot: 0h8m4s  (53.7%)68.6%  lr: 0.023183  loss: 0.001887  eta: 0h6m  tot: 0h8m4s  (53.7%)69.6%  lr: 0.023023  loss: 0.001892  eta: 0h6m  tot: 0h8m5s  (53.9%)69.9%  lr: 0.022983  loss: 0.001893  eta: 0h6m  tot: 0h8m6s  (54.0%)70.1%  lr: 0.022953  loss: 0.001895  eta: 0h6m  tot: 0h8m6s  (54.0%)70.3%  lr: 0.022943  loss: 0.001893  eta: 0h6m  tot: 0h8m6s  (54.1%)73.2%  lr: 0.022653  loss: 0.001892  eta: 0h6m  tot: 0h8m12s  (54.6%)74.3%  lr: 0.022533  loss: 0.001887  eta: 0h6m  tot: 0h8m13s  (54.9%)%  lr: 0.022483  loss: 0.001885  eta: 0h6m  tot: 0h8m14s  (54.9%)74.7%  lr: 0.022453  loss: 0.001886  eta: 0h6m  tot: 0h8m14s  (54.9%)75.6%  lr: 0.022322  loss: 0.001885  eta: 0h6m  tot: 0h8m15s  (55.1%)76.3%  lr: 0.022262  loss: 0.001883  eta: 0h6m  tot: 0h8m16s  (55.3%)76.7%  lr: 0.022202  loss: 0.001883  eta: 0h6m  tot: 0h8m17s  (55.3%)77.0%  lr: 0.022162  loss: 0.001881  eta: 0h6m  tot: 0h8m18s  (55.4%)78.8%  lr: 0.021982  loss: 0.001880  eta: 0h6m  tot: 0h8m21s  (55.8%)80.0%  lr: 0.021892  loss: 0.001878  eta: 0h6m  tot: 0h8m23s  (56.0%)80.2%  lr: 0.021872  loss: 0.001877  eta: 0h6m  tot: 0h8m23s  (56.0%)80.6%  lr: 0.021842  loss: 0.001875  eta: 0h6m  tot: 0h8m23s  (56.1%)80.8%  lr: 0.021832  loss: 0.001877  eta: 0h6m  tot: 0h8m24s  (56.2%)Epoch: 100.0%  lr: 0.020000  loss: 0.001880  eta: 0h5m  tot: 0h8m54s  (60.0%)1.1%  lr: 0.021782  loss: 0.001880  eta: 0h6m  tot: 0h8m24s  (56.2%)82.3%  lr: 0.021662  loss: 0.001877  eta: 0h6m  tot: 0h8m26s  (56.5%)82.7%  lr: 0.021622  loss: 0.001874  eta: 0h6m  tot: 0h8m27s  (56.5%)83.7%  lr: 0.021532  loss: 0.001868  eta: 0h6m  tot: 0h8m28s  (56.7%)84.6%  lr: 0.021392  loss: 0.001867  eta: 0h6m  tot: 0h8m30s  (56.9%)85.0%  lr: 0.021331  loss: 0.001868  eta: 0h6m  tot: 0h8m31s  (57.0%)85.3%  lr: 0.021291  loss: 0.001867  eta: 0h6m  tot: 0h8m31s  (57.1%)87.3%  lr: 0.021091  loss: 0.001872  eta: 0h6m  tot: 0h8m34s  (57.5%)89.3%  lr: 0.020911  loss: 0.001876  eta: 0h6m  tot: 0h8m38s  (57.9%)89.6%  lr: 0.020871  loss: 0.001877  eta: 0h6m  tot: 0h8m38s  (57.9%)89.7%  lr: 0.020861  loss: 0.001878  eta: 0h6m  tot: 0h8m38s  (57.9%)%  lr: 0.020811  loss: 0.001880  eta: 0h6m  tot: 0h8m40s  (58.1%)90.8%  lr: 0.020811  loss: 0.001880  eta: 0h6m  tot: 0h8m40s  (58.2%)91.4%  lr: 0.020741  loss: 0.001881  eta: 0h5m  tot: 0h8m41s  (58.3%)92.8%  lr: 0.020661  loss: 0.001880  eta: 0h5m  tot: 0h8m44s  (58.6%)94.7%  lr: 0.020461  loss: 0.001880  eta: 0h5m  tot: 0h8m47s  (58.9%)95.1%  lr: 0.020421  loss: 0.001878  eta: 0h5m  tot: 0h8m47s  (59.0%)95.2%  lr: 0.020421  loss: 0.001877  eta: 0h5m  tot: 0h8m48s  (59.0%)95.3%  lr: 0.020411  loss: 0.001876  eta: 0h5m  tot: 0h8m48s  (59.1%)96.0%  lr: 0.020340  loss: 0.001876  eta: 0h5m  tot: 0h8m49s  (59.2%)96.7%  lr: 0.020230  loss: 0.001879  eta: 0h5m  tot: 0h8m50s  (59.3%)97.1%  lr: 0.020190  loss: 0.001879  eta: 0h5m  tot: 0h8m51s  (59.4%)97.4%  lr: 0.020160  loss: 0.001877  eta: 0h5m  tot: 0h8m51s  (59.5%)97.9%  lr: 0.020090  loss: 0.001878  eta: 0h5m  tot: 0h8m52s  (59.6%)97.9%  lr: 0.020080  loss: 0.001879  eta: 0h5m  tot: 0h8m52s  (59.6%)98.1%  lr: 0.020070  loss: 0.001879  eta: 0h5m  tot: 0h8m52s  (59.6%)\n",
      " ---+++                Epoch    2 Train error : 0.00188950 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69.1%  lr: 0.012803  loss: 0.001570  eta: 0h3m  tot: 0h10m46s  (73.8%)3%  lr: 0.019980  loss: 0.001007  eta: 0h5m  tot: 0h8m55s  (60.1%)0.4%  lr: 0.019970  loss: 0.000893  eta: 0h5m  tot: 0h8m55s  (60.1%)1.3%  lr: 0.019880  loss: 0.001148  eta: 0h5m  tot: 0h8m57s  (60.3%)2.5%  lr: 0.019770  loss: 0.001289  eta: 0h5m  tot: 0h8m59s  (60.5%)2.7%  lr: 0.019760  loss: 0.001342  eta: 0h5m  tot: 0h8m59s  (60.5%)4.8%  lr: 0.019590  loss: 0.001386  eta: 0h5m  tot: 0h9m2s  (61.0%)4.9%  lr: 0.019590  loss: 0.001407  eta: 0h5m  tot: 0h9m2s  (61.0%)%  lr: 0.019580  loss: 0.001464  eta: 0h5m  tot: 0h9m3s  (61.0%)5.3%  lr: 0.019540  loss: 0.001475  eta: 0h5m  tot: 0h9m3s  (61.1%)5.9%  lr: 0.019469  loss: 0.001460  eta: 0h5m  tot: 0h9m4s  (61.2%)6.2%  lr: 0.019439  loss: 0.001458  eta: 0h5m  tot: 0h9m5s  (61.2%)6.6%  lr: 0.019419  loss: 0.001512  eta: 0h5m  tot: 0h9m5s  (61.3%)6.7%  lr: 0.019389  loss: 0.001510  eta: 0h5m  tot: 0h9m6s  (61.3%)7.0%  lr: 0.019339  loss: 0.001505  eta: 0h5m  tot: 0h9m6s  (61.4%)7.3%  lr: 0.019259  loss: 0.001508  eta: 0h5m  tot: 0h9m7s  (61.5%)8.0%  lr: 0.019139  loss: 0.001483  eta: 0h5m  tot: 0h9m8s  (61.6%)8.3%  lr: 0.019129  loss: 0.001480  eta: 0h5m  tot: 0h9m9s  (61.7%)8.5%  lr: 0.019119  loss: 0.001486  eta: 0h5m  tot: 0h9m9s  (61.7%)8.6%  lr: 0.019109  loss: 0.001495  eta: 0h5m  tot: 0h9m9s  (61.7%)9.7%  lr: 0.018979  loss: 0.001491  eta: 0h5m  tot: 0h9m11s  (61.9%)11.0%  lr: 0.018829  loss: 0.001515  eta: 0h5m  tot: 0h9m13s  (62.2%)11.9%  lr: 0.018639  loss: 0.001521  eta: 0h5m  tot: 0h9m14s  (62.4%)12.1%  lr: 0.018589  loss: 0.001530  eta: 0h5m  tot: 0h9m14s  (62.4%)12.2%  lr: 0.018589  loss: 0.001526  eta: 0h5m  tot: 0h9m14s  (62.4%)12.8%  lr: 0.018519  loss: 0.001529  eta: 0h5m  tot: 0h9m15s  (62.6%)13.6%  lr: 0.018448  loss: 0.001560  eta: 0h5m  tot: 0h9m17s  (62.7%)15.7%  lr: 0.018258  loss: 0.001544  eta: 0h4m  tot: 0h9m20s  (63.1%)15.8%  lr: 0.018248  loss: 0.001549  eta: 0h4m  tot: 0h9m20s  (63.2%)16.2%  lr: 0.018238  loss: 0.001542  eta: 0h4m  tot: 0h9m20s  (63.2%)16.7%  lr: 0.018208  loss: 0.001548  eta: 0h4m  tot: 0h9m21s  (63.3%)17.7%  lr: 0.018108  loss: 0.001549  eta: 0h4m  tot: 0h9m23s  (63.5%)%  lr: 0.018078  loss: 0.001551  eta: 0h4m  tot: 0h9m24s  (63.7%)18.4%  lr: 0.018048  loss: 0.001548  eta: 0h4m  tot: 0h9m24s  (63.7%)18.5%  lr: 0.018048  loss: 0.001557  eta: 0h4m  tot: 0h9m24s  (63.7%)18.5%  lr: 0.018038  loss: 0.001563  eta: 0h4m  tot: 0h9m24s  (63.7%)19.5%  lr: 0.017918  loss: 0.001569  eta: 0h4m  tot: 0h9m26s  (63.9%)19.7%  lr: 0.017898  loss: 0.001561  eta: 0h4m  tot: 0h9m26s  (63.9%)20.3%  lr: 0.017818  loss: 0.001565  eta: 0h4m  tot: 0h9m27s  (64.1%)%  lr: 0.017818  loss: 0.001573  eta: 0h4m  tot: 0h9m27s  (64.1%)21.2%  lr: 0.017738  loss: 0.001575  eta: 0h4m  tot: 0h9m28s  (64.2%)21.4%  lr: 0.017728  loss: 0.001571  eta: 0h4m  tot: 0h9m29s  (64.3%)22.1%  lr: 0.017638  loss: 0.001567  eta: 0h4m  tot: 0h9m30s  (64.4%)22.4%  lr: 0.017628  loss: 0.001556  eta: 0h4m  tot: 0h9m30s  (64.5%)22.9%  lr: 0.017598  loss: 0.001553  eta: 0h4m  tot: 0h9m31s  (64.6%)24.9%  lr: 0.017417  loss: 0.001533  eta: 0h4m  tot: 0h9m34s  (65.0%)25.3%  lr: 0.017357  loss: 0.001530  eta: 0h4m  tot: 0h9m35s  (65.1%)26.4%  lr: 0.017257  loss: 0.001522  eta: 0h4m  tot: 0h9m37s  (65.3%)26.9%  lr: 0.017197  loss: 0.001530  eta: 0h4m  tot: 0h9m38s  (65.4%)27.0%  lr: 0.017177  loss: 0.001531  eta: 0h4m  tot: 0h9m38s  (65.4%)28.7%  lr: 0.017027  loss: 0.001545  eta: 0h4m  tot: 0h9m41s  (65.7%)29.8%  lr: 0.016927  loss: 0.001554  eta: 0h4m  tot: 0h9m42s  (66.0%)31.0%  lr: 0.016797  loss: 0.001553  eta: 0h4m  tot: 0h9m44s  (66.2%)31.5%  lr: 0.016757  loss: 0.001556  eta: 0h4m  tot: 0h9m45s  (66.3%)31.7%  lr: 0.016757  loss: 0.001556  eta: 0h4m  tot: 0h9m45s  (66.3%)31.8%  lr: 0.016747  loss: 0.001555  eta: 0h4m  tot: 0h9m46s  (66.4%)%  lr: 0.016637  loss: 0.001553  eta: 0h4m  tot: 0h9m47s  (66.5%)33.2%  lr: 0.016577  loss: 0.001553  eta: 0h4m  tot: 0h9m48s  (66.6%)33.5%  lr: 0.016567  loss: 0.001556  eta: 0h4m  tot: 0h9m48s  (66.7%)33.8%  lr: 0.016477  loss: 0.001571  eta: 0h4m  tot: 0h9m49s  (66.8%)34.5%  lr: 0.016386  loss: 0.001568  eta: 0h4m  tot: 0h9m50s  (66.9%)35.0%  lr: 0.016336  loss: 0.001573  eta: 0h4m  tot: 0h9m51s  (67.0%)35.1%  lr: 0.016336  loss: 0.001572  eta: 0h4m  tot: 0h9m51s  (67.0%)35.7%  lr: 0.016326  loss: 0.001573  eta: 0h4m  tot: 0h9m52s  (67.1%)35.8%  lr: 0.016306  loss: 0.001573  eta: 0h4m  tot: 0h9m52s  (67.2%)36.1%  lr: 0.016276  loss: 0.001571  eta: 0h4m  tot: 0h9m52s  (67.2%)36.5%  lr: 0.016186  loss: 0.001574  eta: 0h4m  tot: 0h9m53s  (67.3%)37.3%  lr: 0.016116  loss: 0.001576  eta: 0h4m  tot: 0h9m54s  (67.5%)37.9%  lr: 0.016086  loss: 0.001580  eta: 0h4m  tot: 0h9m55s  (67.6%)38.3%  lr: 0.016016  loss: 0.001586  eta: 0h4m  tot: 0h9m56s  (67.7%)38.4%  lr: 0.016006  loss: 0.001587  eta: 0h4m  tot: 0h9m56s  (67.7%)h9m57s  (67.8%)%  lr: 0.015966  loss: 0.001584  eta: 0h4m  tot: 0h9m57s  (67.8%)  loss: 0.001583  eta: 0h4m  tot: 0h9m57s  (67.8%)39.9%  lr: 0.015906  loss: 0.001574  eta: 0h4m  tot: 0h9m58s  (68.0%)40.1%  lr: 0.015866  loss: 0.001569  eta: 0h4m  tot: 0h9m59s  (68.0%)40.7%  lr: 0.015836  loss: 0.001568  eta: 0h4m  tot: 0h10m0s  (68.1%)42.3%  lr: 0.015636  loss: 0.001572  eta: 0h4m  tot: 0h10m3s  (68.5%)42.9%  lr: 0.015586  loss: 0.001573  eta: 0h4m  tot: 0h10m3s  (68.6%)%  lr: 0.015536  loss: 0.001575  eta: 0h4m  tot: 0h10m5s  (68.8%)43.9%  lr: 0.015536  loss: 0.001574  eta: 0h4m  tot: 0h10m6s  (68.8%)45.5%  lr: 0.015425  loss: 0.001586  eta: 0h4m  tot: 0h10m8s  (69.1%)45.7%  lr: 0.015385  loss: 0.001584  eta: 0h4m  tot: 0h10m8s  (69.1%)45.9%  lr: 0.015365  loss: 0.001583  eta: 0h4m  tot: 0h10m9s  (69.2%)46.7%  lr: 0.015295  loss: 0.001589  eta: 0h4m  tot: 0h10m10s  (69.3%)47.0%  lr: 0.015245  loss: 0.001591  eta: 0h4m  tot: 0h10m10s  (69.4%)47.8%  lr: 0.015165  loss: 0.001586  eta: 0h4m  tot: 0h10m12s  (69.6%)48.2%  lr: 0.015095  loss: 0.001590  eta: 0h4m  tot: 0h10m12s  (69.6%)48.3%  lr: 0.015095  loss: 0.001598  eta: 0h4m  tot: 0h10m13s  (69.7%)48.4%  lr: 0.015085  loss: 0.001597  eta: 0h4m  tot: 0h10m13s  (69.7%)49.8%  lr: 0.014985  loss: 0.001595  eta: 0h4m  tot: 0h10m15s  (70.0%)50.3%  lr: 0.014945  loss: 0.001598  eta: 0h4m  tot: 0h10m16s  (70.1%)50.9%  lr: 0.014895  loss: 0.001602  eta: 0h4m  tot: 0h10m17s  (70.2%)51.9%  lr: 0.014795  loss: 0.001596  eta: 0h3m  tot: 0h10m18s  (70.4%)52.9%  lr: 0.014675  loss: 0.001589  eta: 0h3m  tot: 0h10m20s  (70.6%)53.1%  lr: 0.014675  loss: 0.001593  eta: 0h3m  tot: 0h10m20s  (70.6%)53.7%  lr: 0.014585  loss: 0.001587  eta: 0h3m  tot: 0h10m21s  (70.7%)54.7%  lr: 0.014475  loss: 0.001589  eta: 0h3m  tot: 0h10m23s  (70.9%)55.5%  lr: 0.014404  loss: 0.001589  eta: 0h3m  tot: 0h10m24s  (71.1%)55.6%  lr: 0.014404  loss: 0.001588  eta: 0h3m  tot: 0h10m24s  (71.1%)55.9%  lr: 0.014354  loss: 0.001586  eta: 0h3m  tot: 0h10m25s  (71.2%)56.1%  lr: 0.014304  loss: 0.001587  eta: 0h3m  tot: 0h10m25s  (71.2%)56.4%  lr: 0.014234  loss: 0.001588  eta: 0h3m  tot: 0h10m25s  (71.3%)56.5%  lr: 0.014214  loss: 0.001587  eta: 0h3m  tot: 0h10m26s  (71.3%)58.0%  lr: 0.014074  loss: 0.001578  eta: 0h3m  tot: 0h10m28s  (71.6%)58.2%  lr: 0.014054  loss: 0.001577  eta: 0h3m  tot: 0h10m28s  (71.6%)59.3%  lr: 0.013914  loss: 0.001574  eta: 0h3m  tot: 0h10m30s  (71.9%)59.9%  lr: 0.013864  loss: 0.001577  eta: 0h3m  tot: 0h10m31s  (72.0%)0.013774  loss: 0.001570  eta: 0h3m  tot: 0h10m32s  (72.1%)61.1%  lr: 0.013734  loss: 0.001571  eta: 0h3m  tot: 0h10m33s  (72.2%)62.3%  lr: 0.013604  loss: 0.001572  eta: 0h3m  tot: 0h10m35s  (72.5%)62.4%  lr: 0.013604  loss: 0.001571  eta: 0h3m  tot: 0h10m35s  (72.5%)62.7%  lr: 0.013534  loss: 0.001571  eta: 0h3m  tot: 0h10m36s  (72.5%)62.7%  lr: 0.013494  loss: 0.001571  eta: 0h3m  tot: 0h10m36s  (72.5%)62.8%  lr: 0.013494  loss: 0.001570  eta: 0h3m  tot: 0h10m36s  (72.6%)63.2%  lr: 0.013474  loss: 0.001571  eta: 0h3m  tot: 0h10m37s  (72.6%)64.5%  lr: 0.013313  loss: 0.001566  eta: 0h3m  tot: 0h10m39s  (72.9%)64.8%  lr: 0.013303  loss: 0.001568  eta: 0h3m  tot: 0h10m39s  (73.0%)66.1%  lr: 0.013223  loss: 0.001573  eta: 0h3m  tot: 0h10m41s  (73.2%)67.9%  lr: 0.012933  loss: 0.001575  eta: 0h3m  tot: 0h10m44s  (73.6%)69.2%  lr: 0.012803  loss: 0.001570  eta: 0h3m  tot: 0h10m47s  (73.8%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010010  loss: 0.001563  eta: 0h2m  tot: 0h11m36s  (80.0%)0.5%  lr: 0.012703  loss: 0.001574  eta: 0h3m  tot: 0h10m48s  (74.1%)71.0%  lr: 0.012663  loss: 0.001571  eta: 0h3m  tot: 0h10m49s  (74.2%)74.5%  lr: 0.012382  loss: 0.001564  eta: 0h3m  tot: 0h10m55s  (74.9%)74.8%  lr: 0.012372  loss: 0.001565  eta: 0h3m  tot: 0h10m56s  (75.0%)74.9%  lr: 0.012372  loss: 0.001564  eta: 0h3m  tot: 0h10m56s  (75.0%)75.3%  lr: 0.012302  loss: 0.001562  eta: 0h3m  tot: 0h10m56s  (75.1%)75.4%  lr: 0.012292  loss: 0.001560  eta: 0h3m  tot: 0h10m57s  (75.1%)75.5%  lr: 0.012292  loss: 0.001560  eta: 0h3m  tot: 0h10m57s  (75.1%)75.7%  lr: 0.012292  loss: 0.001564  eta: 0h3m  tot: 0h10m57s  (75.1%)75.9%  lr: 0.012282  loss: 0.001565  eta: 0h3m  tot: 0h10m57s  (75.2%)77.1%  lr: 0.012182  loss: 0.001567  eta: 0h3m  tot: 0h10m59s  (75.4%)77.2%  lr: 0.012172  loss: 0.001568  eta: 0h3m  tot: 0h10m59s  (75.4%)77.4%  lr: 0.012172  loss: 0.001570  eta: 0h3m  tot: 0h11m0s  (75.5%)78.0%  lr: 0.012112  loss: 0.001569  eta: 0h3m  tot: 0h11m1s  (75.6%)78.2%  lr: 0.012102  loss: 0.001570  eta: 0h3m  tot: 0h11m1s  (75.6%)78.9%  lr: 0.012022  loss: 0.001568  eta: 0h3m  tot: 0h11m2s  (75.8%)79.4%  lr: 0.011952  loss: 0.001568  eta: 0h3m  tot: 0h11m3s  (75.9%)80.0%  lr: 0.011892  loss: 0.001567  eta: 0h3m  tot: 0h11m4s  (76.0%)80.1%  lr: 0.011882  loss: 0.001565  eta: 0h3m  tot: 0h11m4s  (76.0%)80.4%  lr: 0.011842  loss: 0.001566  eta: 0h3m  tot: 0h11m5s  (76.1%)  loss: 0.001563  eta: 0h3m  tot: 0h11m6s  (76.2%)81.3%  lr: 0.011782  loss: 0.001563  eta: 0h3m  tot: 0h11m6s  (76.3%)81.9%  lr: 0.011712  loss: 0.001565  eta: 0h3m  tot: 0h11m7s  (76.4%)83.1%  lr: 0.011612  loss: 0.001567  eta: 0h3m  tot: 0h11m9s  (76.6%)83.5%  lr: 0.011602  loss: 0.001567  eta: 0h3m  tot: 0h11m10s  (76.7%)83.9%  lr: 0.011552  loss: 0.001567  eta: 0h3m  tot: 0h11m10s  (76.8%)84.1%  lr: 0.011532  loss: 0.001567  eta: 0h3m  tot: 0h11m11s  (76.8%)%  lr: 0.011522  loss: 0.001567  eta: 0h3m  tot: 0h11m11s  (76.8%)86.2%  lr: 0.011321  loss: 0.001567  eta: 0h3m  tot: 0h11m14s  (77.2%)86.8%  lr: 0.011251  loss: 0.001567  eta: 0h3m  tot: 0h11m15s  (77.4%)87.1%  lr: 0.011221  loss: 0.001571  eta: 0h3m  tot: 0h11m16s  (77.4%)87.3%  lr: 0.011161  loss: 0.001571  eta: 0h3m  tot: 0h11m16s  (77.5%)88.0%  lr: 0.011121  loss: 0.001569  eta: 0h3m  tot: 0h11m17s  (77.6%)88.1%  lr: 0.011091  loss: 0.001570  eta: 0h3m  tot: 0h11m18s  (77.6%)88.4%  lr: 0.011051  loss: 0.001570  eta: 0h3m  tot: 0h11m18s  (77.7%)88.5%  lr: 0.011041  loss: 0.001571  eta: 0h3m  tot: 0h11m18s  (77.7%)88.6%  lr: 0.011041  loss: 0.001573  eta: 0h3m  tot: 0h11m18s  (77.7%)88.9%  lr: 0.011011  loss: 0.001573  eta: 0h3m  tot: 0h11m19s  (77.8%)88.9%  lr: 0.011011  loss: 0.001576  eta: 0h3m  tot: 0h11m19s  (77.8%)  lr: 0.010971  loss: 0.001575  eta: 0h3m  tot: 0h11m19s  (77.8%)89.7%  lr: 0.010941  loss: 0.001575  eta: 0h2m  tot: 0h11m20s  (77.9%)90.7%  lr: 0.010851  loss: 0.001575  eta: 0h2m  tot: 0h11m22s  (78.1%)91.0%  lr: 0.010821  loss: 0.001575  eta: 0h2m  tot: 0h11m23s  (78.2%)92.2%  lr: 0.010681  loss: 0.001569  eta: 0h2m  tot: 0h11m24s  (78.4%)92.4%  lr: 0.010641  loss: 0.001568  eta: 0h2m  tot: 0h11m25s  (78.5%)92.7%  lr: 0.010621  loss: 0.001569  eta: 0h2m  tot: 0h11m25s  (78.5%)93.4%  lr: 0.010561  loss: 0.001566  eta: 0h2m  tot: 0h11m26s  (78.7%)94.1%  lr: 0.010481  loss: 0.001563  eta: 0h2m  tot: 0h11m27s  (78.8%)94.2%  lr: 0.010461  loss: 0.001564  eta: 0h2m  tot: 0h11m28s  (78.8%)94.6%  lr: 0.010431  loss: 0.001565  eta: 0h2m  tot: 0h11m28s  (78.9%)  loss: 0.001567  eta: 0h2m  tot: 0h11m30s  (79.2%)97.5%  lr: 0.010150  loss: 0.001563  eta: 0h2m  tot: 0h11m33s  (79.5%)97.8%  lr: 0.010130  loss: 0.001565  eta: 0h2m  tot: 0h11m33s  (79.6%)98.0%  lr: 0.010110  loss: 0.001565  eta: 0h2m  tot: 0h11m34s  (79.6%)98.6%  lr: 0.010080  loss: 0.001564  eta: 0h2m  tot: 0h11m34s  (79.7%)\n",
      " ---+++                Epoch    3 Train error : 0.00156163 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87.8%  lr: 0.001472  loss: 0.001429  eta: <1min   tot: 0h13m56s  (97.6%).3%  lr: 0.009980  loss: 0.000938  eta: 0h1m  tot: 0h11m36s  (80.1%)0.4%  lr: 0.009940  loss: 0.001063  eta: 0h2m  tot: 0h11m36s  (80.1%)0.5%  lr: 0.009940  loss: 0.001074  eta: 0h2m  tot: 0h11m37s  (80.1%)%  lr: 0.009860  loss: 0.001321  eta: 0h2m  tot: 0h11m38s  (80.3%)2.2%  lr: 0.009810  loss: 0.001340  eta: 0h2m  tot: 0h11m39s  (80.4%)2.8%  lr: 0.009770  loss: 0.001360  eta: 0h2m  tot: 0h11m40s  (80.6%)3.3%  lr: 0.009730  loss: 0.001314  eta: 0h2m  tot: 0h11m41s  (80.7%)4.8%  lr: 0.009600  loss: 0.001351  eta: 0h2m  tot: 0h11m44s  (81.0%)4.9%  lr: 0.009600  loss: 0.001368  eta: 0h2m  tot: 0h11m44s  (81.0%)6.9%  lr: 0.009449  loss: 0.001351  eta: 0h2m  tot: 0h11m47s  (81.4%)7.7%  lr: 0.009379  loss: 0.001375  eta: 0h2m  tot: 0h11m48s  (81.5%)8.6%  lr: 0.009309  loss: 0.001388  eta: 0h2m  tot: 0h11m49s  (81.7%)  loss: 0.001388  eta: 0h2m  tot: 0h11m50s  (81.8%)8.9%  lr: 0.009299  loss: 0.001387  eta: 0h2m  tot: 0h11m50s  (81.8%)9.5%  lr: 0.009229  loss: 0.001401  eta: 0h2m  tot: 0h11m51s  (81.9%)9.7%  lr: 0.009219  loss: 0.001389  eta: 0h2m  tot: 0h11m51s  (81.9%)11.0%  lr: 0.009109  loss: 0.001381  eta: 0h2m  tot: 0h11m53s  (82.2%)11.2%  lr: 0.009099  loss: 0.001382  eta: 0h2m  tot: 0h11m53s  (82.2%)14.5%  lr: 0.008839  loss: 0.001371  eta: 0h2m  tot: 0h11m59s  (82.9%)14.9%  lr: 0.008819  loss: 0.001365  eta: 0h2m  tot: 0h11m59s  (83.0%)15.2%  lr: 0.008789  loss: 0.001364  eta: 0h2m  tot: 0h12m0s  (83.0%)15.3%  lr: 0.008779  loss: 0.001370  eta: 0h2m  tot: 0h12m0s  (83.1%)16.2%  lr: 0.008729  loss: 0.001380  eta: 0h2m  tot: 0h12m1s  (83.2%)16.7%  lr: 0.008649  loss: 0.001402  eta: 0h2m  tot: 0h12m2s  (83.3%)17.4%  lr: 0.008569  loss: 0.001407  eta: 0h2m  tot: 0h12m3s  (83.5%)18.1%  lr: 0.008468  loss: 0.001406  eta: 0h2m  tot: 0h12m4s  (83.6%)18.9%  lr: 0.008308  loss: 0.001401  eta: 0h2m  tot: 0h12m6s  (83.8%)19.6%  lr: 0.008248  loss: 0.001403  eta: 0h2m  tot: 0h12m7s  (83.9%)21.8%  lr: 0.008038  loss: 0.001404  eta: 0h2m  tot: 0h12m10s  (84.4%)23.0%  lr: 0.007878  loss: 0.001409  eta: 0h2m  tot: 0h12m12s  (84.6%)23.9%  lr: 0.007828  loss: 0.001411  eta: 0h1m  tot: 0h12m14s  (84.8%)24.1%  lr: 0.007818  loss: 0.001405  eta: 0h1m  tot: 0h12m14s  (84.8%)25.3%  lr: 0.007728  loss: 0.001414  eta: 0h1m  tot: 0h12m16s  (85.1%)25.7%  lr: 0.007678  loss: 0.001413  eta: 0h1m  tot: 0h12m16s  (85.1%)26.3%  lr: 0.007598  loss: 0.001412  eta: 0h1m  tot: 0h12m17s  (85.3%)26.6%  lr: 0.007578  loss: 0.001411  eta: 0h1m  tot: 0h12m18s  (85.3%)26.7%  lr: 0.007558  loss: 0.001409  eta: 0h1m  tot: 0h12m18s  (85.3%)27.2%  lr: 0.007498  loss: 0.001409  eta: 0h1m  tot: 0h12m19s  (85.4%)27.5%  lr: 0.007468  loss: 0.001407  eta: 0h1m  tot: 0h12m19s  (85.5%)27.7%  lr: 0.007417  loss: 0.001406  eta: 0h1m  tot: 0h12m19s  (85.5%)28.0%  lr: 0.007377  loss: 0.001403  eta: 0h1m  tot: 0h12m20s  (85.6%)29.3%  lr: 0.007277  loss: 0.001392  eta: 0h1m  tot: 0h12m22s  (85.9%)29.9%  lr: 0.007247  loss: 0.001391  eta: 0h1m  tot: 0h12m23s  (86.0%)30.2%  lr: 0.007247  loss: 0.001393  eta: 0h1m  tot: 0h12m23s  (86.0%)31.3%  lr: 0.007127  loss: 0.001383  eta: 0h1m  tot: 0h12m25s  (86.3%)  loss: 0.001384  eta: 0h1m  tot: 0h12m27s  (86.6%)32.9%  lr: 0.007007  loss: 0.001389  eta: 0h1m  tot: 0h12m28s  (86.6%)33.1%  lr: 0.006997  loss: 0.001385  eta: 0h1m  tot: 0h12m28s  (86.6%)33.2%  lr: 0.006997  loss: 0.001383  eta: 0h1m  tot: 0h12m28s  (86.6%)33.4%  lr: 0.006997  loss: 0.001380  eta: 0h1m  tot: 0h12m29s  (86.7%)38.2%  lr: 0.006527  loss: 0.001388  eta: 0h1m  tot: 0h12m37s  (87.6%)38.5%  lr: 0.006477  loss: 0.001390  eta: 0h1m  tot: 0h12m37s  (87.7%)39.8%  lr: 0.006346  loss: 0.001378  eta: 0h1m  tot: 0h12m40s  (88.0%)  loss: 0.001375  eta: 0h1m  tot: 0h12m40s  (88.0%)40.3%  lr: 0.006286  loss: 0.001375  eta: 0h1m  tot: 0h12m41s  (88.1%)41.1%  lr: 0.006216  loss: 0.001378  eta: 0h1m  tot: 0h12m42s  (88.2%)41.5%  lr: 0.006136  loss: 0.001378  eta: 0h1m  tot: 0h12m43s  (88.3%)41.7%  lr: 0.006096  loss: 0.001380  eta: 0h1m  tot: 0h12m43s  (88.3%)42.5%  lr: 0.005996  loss: 0.001385  eta: 0h1m  tot: 0h12m44s  (88.5%)43.2%  lr: 0.005886  loss: 0.001385  eta: 0h1m  tot: 0h12m45s  (88.6%)  lr: 0.005736  loss: 0.001395  eta: 0h1m  tot: 0h12m47s  (88.9%)47.4%  lr: 0.005486  loss: 0.001404  eta: 0h1m  tot: 0h12m52s  (89.5%)47.7%  lr: 0.005456  loss: 0.001402  eta: 0h1m  tot: 0h12m52s  (89.5%)48.3%  lr: 0.005385  loss: 0.001408  eta: 0h1m  tot: 0h12m53s  (89.7%)48.7%  lr: 0.005335  loss: 0.001411  eta: 0h1m  tot: 0h12m54s  (89.7%)49.2%  lr: 0.005275  loss: 0.001409  eta: 0h1m  tot: 0h12m55s  (89.8%)51.0%  lr: 0.005155  loss: 0.001412  eta: 0h1m  tot: 0h12m57s  (90.2%)0.005115  loss: 0.001409  eta: 0h1m  tot: 0h12m58s  (90.3%)%  lr: 0.004965  loss: 0.001408  eta: 0h1m  tot: 0h13m0s  (90.5%)52.5%  lr: 0.004945  loss: 0.001408  eta: 0h1m  tot: 0h13m0s  (90.5%)52.8%  lr: 0.004905  loss: 0.001406  eta: 0h1m  tot: 0h13m0s  (90.6%)53.3%  lr: 0.004875  loss: 0.001408  eta: 0h1m  tot: 0h13m1s  (90.7%)53.7%  lr: 0.004835  loss: 0.001406  eta: 0h1m  tot: 0h13m2s  (90.7%)54.8%  lr: 0.004735  loss: 0.001406  eta: 0h1m  tot: 0h13m3s  (91.0%)55.3%  lr: 0.004685  loss: 0.001407  eta: 0h1m  tot: 0h13m4s  (91.1%)55.5%  lr: 0.004665  loss: 0.001408  eta: 0h1m  tot: 0h13m5s  (91.1%)56.2%  lr: 0.004585  loss: 0.001417  eta: 0h1m  tot: 0h13m6s  (91.2%)56.7%  lr: 0.004575  loss: 0.001416  eta: 0h1m  tot: 0h13m7s  (91.3%)57.5%  lr: 0.004505  loss: 0.001424  eta: 0h1m  tot: 0h13m8s  (91.5%)59.2%  lr: 0.004394  loss: 0.001424  eta: 0h1m  tot: 0h13m10s  (91.8%)%  lr: 0.004354  loss: 0.001421  eta: 0h1m  tot: 0h13m11s  (91.9%)60.0%  lr: 0.004264  loss: 0.001419  eta: 0h1m  tot: 0h13m12s  (92.0%)61.0%  lr: 0.004154  loss: 0.001420  eta: 0h1m  tot: 0h13m13s  (92.2%)61.1%  lr: 0.004154  loss: 0.001419  eta: 0h1m  tot: 0h13m14s  (92.2%)0.004124  loss: 0.001418  eta: 0h1m  tot: 0h13m14s  (92.3%)62.3%  lr: 0.004034  loss: 0.001425  eta: 0h1m  tot: 0h13m16s  (92.5%)63.6%  lr: 0.003884  loss: 0.001425  eta: <1min   tot: 0h13m18s  (92.7%)65.4%  lr: 0.003784  loss: 0.001433  eta: <1min   tot: 0h13m20s  (93.1%)67.1%  lr: 0.003594  loss: 0.001427  eta: <1min   tot: 0h13m23s  (93.4%)68.4%  lr: 0.003373  loss: 0.001430  eta: <1min   tot: 0h13m25s  (93.7%)69.6%  lr: 0.003243  loss: 0.001433  eta: <1min   tot: 0h13m27s  (93.9%)70.1%  lr: 0.003183  loss: 0.001433  eta: <1min   tot: 0h13m28s  (94.0%)70.8%  lr: 0.003153  loss: 0.001433  eta: <1min   tot: 0h13m29s  (94.2%)71.2%  lr: 0.003093  loss: 0.001433  eta: <1min   tot: 0h13m30s  (94.2%)71.9%  lr: 0.002983  loss: 0.001432  eta: <1min   tot: 0h13m31s  (94.4%)72.1%  lr: 0.002963  loss: 0.001431  eta: <1min   tot: 0h13m31s  (94.4%)73.0%  lr: 0.002833  loss: 0.001435  eta: <1min   tot: 0h13m33s  (94.6%)73.2%  lr: 0.002833  loss: 0.001435  eta: <1min   tot: 0h13m33s  (94.6%)%  lr: 0.002753  loss: 0.001432  eta: <1min   tot: 0h13m34s  (94.8%)74.1%  lr: 0.002753  loss: 0.001438  eta: <1min   tot: 0h13m34s  (94.8%)75.5%  lr: 0.002623  loss: 0.001440  eta: <1min   tot: 0h13m37s  (95.1%)75.8%  lr: 0.002603  loss: 0.001438  eta: <1min   tot: 0h13m37s  (95.2%)76.4%  lr: 0.002573  loss: 0.001439  eta: <1min   tot: 0h13m38s  (95.3%)77.5%  lr: 0.002463  loss: 0.001441  eta: <1min   tot: 0h13m40s  (95.5%)79.7%  lr: 0.002242  loss: 0.001443  eta: <1min   tot: 0h13m43s  (95.9%)79.9%  lr: 0.002232  loss: 0.001443  eta: <1min   tot: 0h13m43s  (96.0%)80.6%  lr: 0.002182  loss: 0.001442  eta: <1min   tot: 0h13m45s  (96.1%)82.4%  lr: 0.002062  loss: 0.001440  eta: <1min   tot: 0h13m47s  (96.5%)82.5%  lr: 0.002052  loss: 0.001440  eta: <1min   tot: 0h13m48s  (96.5%)82.9%  lr: 0.002002  loss: 0.001438  eta: <1min   tot: 0h13m48s  (96.6%)83.3%  lr: 0.001952  loss: 0.001436  eta: <1min   tot: 0h13m49s  (96.7%)84.4%  lr: 0.001852  loss: 0.001434  eta: <1min   tot: 0h13m50s  (96.9%)84.6%  lr: 0.001842  loss: 0.001434  eta: <1min   tot: 0h13m51s  (96.9%)86.0%  lr: 0.001672  loss: 0.001428  eta: <1min   tot: 0h13m53s  (97.2%)86.9%  lr: 0.001562  loss: 0.001426  eta: <1min   tot: 0h13m54s  (97.4%)87.7%  lr: 0.001472  loss: 0.001429  eta: <1min   tot: 0h13m55s  (97.5%)87.9%  lr: 0.001472  loss: 0.001429  eta: <1min   tot: 0h13m56s  (97.6%)Epoch: 99.9%  lr: 0.000190  loss: 0.001425  eta: <1min   tot: 0h14m15s  (100.0%)8.2%  lr: 0.001442  loss: 0.001428  eta: <1min   tot: 0h13m56s  (97.6%)88.9%  lr: 0.001381  loss: 0.001426  eta: <1min   tot: 0h13m57s  (97.8%)90.5%  lr: 0.001171  loss: 0.001424  eta: <1min   tot: 0h14m0s  (98.1%)91.3%  lr: 0.001111  loss: 0.001425  eta: <1min   tot: 0h14m1s  (98.3%)91.5%  lr: 0.001081  loss: 0.001426  eta: <1min   tot: 0h14m1s  (98.3%)91.6%  lr: 0.001071  loss: 0.001426  eta: <1min   tot: 0h14m1s  (98.3%)92.1%  lr: 0.001021  loss: 0.001424  eta: <1min   tot: 0h14m2s  (98.4%)92.5%  lr: 0.000971  loss: 0.001426  eta: <1min   tot: 0h14m3s  (98.5%)93.2%  lr: 0.000891  loss: 0.001428  eta: <1min   tot: 0h14m4s  (98.6%)93.4%  lr: 0.000851  loss: 0.001427  eta: <1min   tot: 0h14m4s  (98.7%)93.6%  lr: 0.000841  loss: 0.001428  eta: <1min   tot: 0h14m5s  (98.7%)94.3%  lr: 0.000791  loss: 0.001429  eta: <1min   tot: 0h14m6s  (98.9%)  loss: 0.001428  eta: <1min   tot: 0h14m7s  (99.0%)96.1%  lr: 0.000561  loss: 0.001430  eta: <1min   tot: 0h14m9s  (99.2%)96.8%  lr: 0.000471  loss: 0.001428  eta: <1min   tot: 0h14m10s  (99.4%)97.1%  lr: 0.000431  loss: 0.001426  eta: <1min   tot: 0h14m10s  (99.4%)%  lr: 0.000400  loss: 0.001425  eta: <1min   tot: 0h14m11s  (99.5%)97.7%  lr: 0.000380  loss: 0.001425  eta: <1min   tot: 0h14m11s  (99.5%)97.9%  lr: 0.000370  loss: 0.001425  eta: <1min   tot: 0h14m12s  (99.6%)98.5%  lr: 0.000330  loss: 0.001426  eta: <1min   tot: 0h14m13s  (99.7%)98.8%  lr: 0.000290  loss: 0.001426  eta: <1min   tot: 0h14m13s  (99.8%)98.8%  lr: 0.000270  loss: 0.001425  eta: <1min   tot: 0h14m13s  (99.8%)98.9%  lr: 0.000260  loss: 0.001426  eta: <1min   tot: 0h14m13s  (99.8%)99.2%  lr: 0.000250  loss: 0.001426  eta: <1min   tot: 0h14m14s  (99.8%)100.0%  lr: 0.000180  loss: 0.001425  eta: <1min   tot: 0h14m15s  (100.0%)\n",
      " ---+++                Epoch    4 Train error : 0.00138475 +++--- ���\n",
      "Saving model to file : starspace_embedding\n",
      "Saving model in tsv format : starspace_embedding.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainFile \"data/prepared_train.tsv\" -model starspace_embedding \\\n",
    "-trainMode 3 -adagrad true -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 \\\n",
    "-verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "starspace_embeddings = {}  \n",
    "for line in open('starspace_embedding.tsv', encoding='utf-8'):\n",
    "    row = line.strip().split('\\t')\n",
    "    starspace_embeddings[row[0]] = np.array(row[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.518 | Hits@   1: 0.518\n",
      "DCG@   5: 0.617 | Hits@   5: 0.701\n",
      "DCG@  10: 0.635 | Hits@  10: 0.757\n",
      "DCG@ 100: 0.666 | Hits@ 100: 0.906\n",
      "DCG@ 500: 0.675 | Hits@ 500: 0.980\n",
      "DCG@1000: 0.678 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 87\t77\t34\t13\t9\t84\t70\t90\t56\t51\t40\t14\t46\t33\t86\t64\t3\t66\t50\t95\t93\t19\t7\t97\t75\t54\t62\t89\t76\t4\t65\t22\t78\t20\t59...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = prepared_test_data ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059027776\n",
      "-0.028727213541666668\n",
      "0.046056111653645836\n",
      "0.08525933159722222\n",
      "0.02430555555555...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 87\t77\t34\t13\t9\t84\t70\t90\t56\t51\t40\t14\t46\t33\t86\t64\t3\t66\t50\t95\t93\t19\t7\t97\t75\t54\t62\t89\t76\t4\t65\t22\t78\t20\t59...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'er.bhavesh@live.com' # EMAIL \n",
    "STUDENT_TOKEN = 'jZ1MEAYyOaY3HGZB' # TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
